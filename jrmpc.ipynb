{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import coreset\n",
    "from source.utilities import subset, custom_sampler\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.addpath('source', nargout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2394db",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bfba7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "dataset_name = 'POKERDVS' # name of dataset: POKERDVS -- NMNIST -- NCARS -- DVSGesture\n",
    "download_dataset = True # downloads the datasets before parsing\n",
    "first_saccade_only = False # specific for N-MNIST (3 saccades 100ms each)\n",
    "subsample = 100 # take a sample of the dataset\n",
    "spatial_histograms = True\n",
    "K = 10\n",
    "coresets = False\n",
    "Np = 1000\n",
    "t_res = 1000\n",
    "\n",
    "# algorithm parameters\n",
    "C = 5\n",
    "maxNumIter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba87f38-147f-4caf-a046-a47511fb9bb1",
   "metadata": {},
   "source": [
    "### generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if dataset_name == 'NCARS': # 304 x 240\n",
    "    train_set = tonic.datasets.NCARS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.NCARS(save_to='./data', train=False, download=download_dataset)\n",
    "if dataset_name == 'POKERDVS': # 35 x 35\n",
    "    train_set = tonic.datasets.POKERDVS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.POKERDVS(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == \"DVSGesture\": # 128 x 128\n",
    "    train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == 'NMNIST': # 34 x 34\n",
    "    train_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=True, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "    test_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=False, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "    \n",
    "x_index = train_set.ordering.find('x')\n",
    "y_index = train_set.ordering.find('y')\n",
    "t_index = train_set.ordering.find('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8042eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_index = subset(train_set, subsample)\n",
    "trainloader = DataLoader(train_set, sampler=custom_sampler(train_index), shuffle=False)\n",
    "\n",
    "test_index = subset(test_set, 100)\n",
    "testloader = DataLoader(test_set, sampler=custom_sampler(test_index), shuffle=False)\n",
    "\n",
    "dirname = \"./data/\"+dataset_name.lower()+\"_\"+datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
    "Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "for i, (events, target) in enumerate(trainloader):\n",
    "    events = events.numpy().squeeze()\n",
    "    data = np.vstack((events[:,t_index]/t_res, events[:,x_index], events[:,y_index])).T\n",
    "    if coresets: data, _ = coreset.generate(data, Np)\n",
    "    np.savetxt(dirname+\"/\"+dataset_name.lower()+\"_%s.txt\" % (i+1), data, delimiter=' ', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "M = len(trainloader)\n",
    "centers = np.asarray(eng.jrmpc(dirname+\"/\"+dataset_name.lower()+'_%d.txt', M, C, 'maxNumIter', maxNumIter, 'gamma', 0.1)).T\n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(centers, data):\n",
    "    labels = np.zeros(data.shape[0], dtype=int)\n",
    "    for i, d in enumerate(data):\n",
    "        labels[i] = np.argmin(np.linalg.norm(centers - d, axis=1)**2)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71d7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_histograms(dataloader, centers, C, dataset):\n",
    "    x_index = dataset.ordering.find(\"x\")\n",
    "    y_index = dataset.ordering.find(\"y\")\n",
    "    t_index = dataset.ordering.find(\"t\")\n",
    "    X = []\n",
    "    Y = np.zeros(len(dataloader))\n",
    "    for i, (events, target) in enumerate(dataloader):\n",
    "        events = events.numpy().squeeze()\n",
    "        data = np.vstack((events[:,t_index]/t_res, events[:,x_index], events[:,y_index])).T\n",
    "        Y_pred = predict(centers, data)\n",
    "        Y[i] = target.numpy()[0]\n",
    "        X.append(np.histogram(Y_pred, bins=np.arange(0, C))[0])\n",
    "    return X, Y\n",
    "\n",
    "def create_spatial_histograms(dataloader, centers, C, dataset, K):\n",
    "    sensor_size = dataset.sensor_size\n",
    "    x_index = dataset.ordering.find(\"x\")\n",
    "    y_index = dataset.ordering.find(\"y\")\n",
    "    t_index = dataset.ordering.find(\"t\")\n",
    "    X = []\n",
    "    Y = np.zeros(len(dataloader))\n",
    "    n_cells = (sensor_size[0] // K+1) * (sensor_size[1] // K+1)\n",
    "    for i, (events, target) in enumerate(dataloader):\n",
    "        events = events.numpy().squeeze()\n",
    "        data = np.vstack((events[:,t_index]/t_res, events[:,x_index], events[:,y_index])).T\n",
    "        Y_pred = predict(centers, data)\n",
    "        Y[i] = target.numpy()[0]\n",
    "        \n",
    "        cells = []\n",
    "        cell_index = 0\n",
    "        for i in range(sensor_size[0] // K +1):\n",
    "            for j in range(sensor_size[1] // K +1):\n",
    "                xs = events[:,x_index]\n",
    "                ys = events[:,y_index]\n",
    "                selection = events[(xs >= i*K) & (xs < i*K+K) & (ys >= j*K) & (ys < j*K+K)]\n",
    "                if len(selection) > 0:\n",
    "                    cells.extend([cell_index] * len(selection))\n",
    "                cell_index += 1\n",
    "        \n",
    "        hists = []\n",
    "        for i in np.arange(n_cells):\n",
    "            selection = (cells == i)\n",
    "            if len(selection) > 0:\n",
    "                hists.append(np.histogram(Y_pred[selection], bins=np.arange(0, C+1))[0])\n",
    "            else:\n",
    "                hists.append(np.zeros(C))\n",
    "        X.append(np.concatenate(hists))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f96021",
   "metadata": {},
   "source": [
    "### pytorch classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.logreg import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# creating histograms from hard clusters\n",
    "if spatial_histograms:\n",
    "    train_features, train_labels = create_spatial_histograms(trainloader, centers, C, train_set, K)\n",
    "    test_features, test_labels = create_spatial_histograms(testloader, centers, C, test_set, K)\n",
    "else:\n",
    "    train_features, train_labels = create_histograms(trainloader, centers, C, train_set)\n",
    "    test_features, test_labels = create_histograms(testloader, centers, C, test_set)\n",
    "\n",
    "# scale features to 0 mean and 1 variance\n",
    "scaler = preprocessing.StandardScaler().fit(train_features)\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "# creating dataloaders\n",
    "training_dataset = TensorDataset(torch.Tensor(train_features),torch.Tensor(train_labels))\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=128)\n",
    "\n",
    "test_dataset = TensorDataset(torch .Tensor(test_features),torch.Tensor(test_labels))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# finding unique classes\n",
    "classes = np.unique(test_labels)\n",
    "\n",
    "# training pytorch logistic regression\n",
    "logreg = LogisticRegression(train_features.shape[1], len(classes), epochs=200, lr=0.01, step_size=30, gamma=1, momentum=0, weight_decay=0)\n",
    "logreg.fit(training_dataloader)\n",
    "score = logreg.score(test_dataloader)*100\n",
    "\n",
    "# print score\n",
    "print(score)\n",
    "\n",
    "# save results\n",
    "p = Path('jrmpc')\n",
    "p.mkdir(exist_ok=True)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
    "filename = dataset_name+'_'+str(score)+'_'+str(C)+'_sp-hist_'+str(spatial_histograms)+'_'+date\n",
    "np.save(p/filename, score)\n",
    "\n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1324609-4432-4057-9317-43dcb5f946c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
