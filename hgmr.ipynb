{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from probreg import gmmtree\n",
    "import tonic\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from pathlib import Path\n",
    "import time\n",
    "import coreset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438f2ab",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'POKERDVS' # name of dataset: POKERDVS -- NMNIST -- NCARS -- DVSGesture\n",
    "download_dataset = True # downloads the datasets before parsing\n",
    "first_saccade_only = False # specific for N-MNIST (3 saccades 100ms each)\n",
    "subsample = 100 # take a sample of the dataset\n",
    "tree_level = 3 # max number of levels for the GMM hierarchy\n",
    "inference_level = 3 # at which level of the tree to do the inference\n",
    "spatial_histograms = True\n",
    "K = 10\n",
    "coresets = False\n",
    "Np = 2**16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82905d1a",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b58528",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if dataset_name == 'NCARS': # 304 x 240\n",
    "    train_set = tonic.datasets.NCARS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.NCARS(save_to='./data', train=False, download=download_dataset)\n",
    "if dataset_name == 'POKERDVS': # 35 x 35\n",
    "    train_set = tonic.datasets.POKERDVS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.POKERDVS(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == \"DVSGesture\": # 128 x 128\n",
    "    train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == 'NMNIST': # 34 x 34\n",
    "    train_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=True, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "    test_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=False, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "    \n",
    "x_index = train_set.ordering.find('x')\n",
    "y_index = train_set.ordering.find('y')\n",
    "t_index = train_set.ordering.find('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset\n",
    "train_index = np.arange(len(train_set))\n",
    "np.random.shuffle(train_index)\n",
    "\n",
    "test_index = np.arange(len(test_set))\n",
    "np.random.shuffle(test_index)\n",
    "\n",
    "if subsample > 0 and subsample < 100:\n",
    "    print(\"Taking %s%% of the dataset\" % subsample)\n",
    "    \n",
    "    # calculate number of samples we want to take\n",
    "    train_samples = np.ceil((subsample * len(train_set)) / 100).astype(int)\n",
    "    test_samples = np.ceil((subsample * len(test_set)) / 100).astype(int)\n",
    "    \n",
    "    # choosing indices of the subset\n",
    "    train_index = train_index[:train_samples]\n",
    "    test_index = test_index[:test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom sampler for torch dataloader\n",
    "class custom_sampler(Sampler):\n",
    "    \"\"\"Samples elements from a given list of indices.\n",
    "    \n",
    "    Arguments:\n",
    "        indices (list): a list of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.num_samples = len(indices)\n",
    "        self.indices = indices\n",
    "     \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = tonic.datasets.DataLoader(train_set, sampler=custom_sampler(train_index), shuffle=False)\n",
    "testloader = tonic.datasets.DataLoader(test_set, sampler=custom_sampler(test_index), shuffle=False)\n",
    "\n",
    "# preparing training dataset\n",
    "X_train = []\n",
    "Y_train = np.zeros(len(trainloader))\n",
    "for i, (events, target) in enumerate(trainloader):\n",
    "    events = events.numpy().squeeze()\n",
    "    data = np.vstack((events[:,t_index], events[:,x_index], events[:,y_index])).T\n",
    "    X_train.append(data)\n",
    "    Y_train[i] = target.numpy()[0]\n",
    "X_train = np.vstack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "if coresets:\n",
    "    X_train, weights = coreset.generate(X_train, Np)\n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8319d55",
   "metadata": {},
   "source": [
    "### running hGMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f744c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model, params, n_nodes = gmmtree.fit(X_train, tree_level=tree_level)\n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1594b16",
   "metadata": {},
   "source": [
    "### preparing extracted features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f95f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_histograms(dataloader, model, n_nodes, inference_level, dataset):\n",
    "    x_index = dataset.ordering.find(\"x\")\n",
    "    y_index = dataset.ordering.find(\"y\")\n",
    "    t_index = dataset.ordering.find(\"t\")\n",
    "    X = []\n",
    "    Y = np.zeros(len(dataloader))\n",
    "    for i, (events, target) in enumerate(dataloader):\n",
    "        events = events.numpy().squeeze()\n",
    "        data = np.vstack((events[:,t_index], events[:,x_index], events[:,y_index])).T\n",
    "        Y_pred = gmmtree.predict(model, data, inference_level)\n",
    "        Y[i] = target.numpy()[0]\n",
    "        \n",
    "        X.append(np.histogram(Y_pred, bins=np.arange(0, n_nodes[inference_level-1]))[0])\n",
    "    return X, Y\n",
    "\n",
    "def create_spatial_histograms(dataloader, model, n_nodes, inference_level, dataset, K):\n",
    "    sensor_size = dataset.sensor_size\n",
    "    x_index = dataset.ordering.find(\"x\")\n",
    "    y_index = dataset.ordering.find(\"y\")\n",
    "    t_index = dataset.ordering.find(\"t\")\n",
    "    X = []\n",
    "    Y = np.zeros(len(dataloader))\n",
    "    n_cells = (sensor_size[0] // K+1) * (sensor_size[1] // K+1)\n",
    "    for i, (events, target) in enumerate(dataloader):\n",
    "        events = events.numpy().squeeze()\n",
    "        data = np.vstack((events[:,t_index], events[:,x_index], events[:,y_index])).T\n",
    "        Y_pred = gmmtree.predict(model, data, inference_level)\n",
    "        Y[i] = target.numpy()[0]\n",
    "        \n",
    "        cells = []\n",
    "        cell_index = 0\n",
    "        for i in range(sensor_size[0] // K +1):\n",
    "            for j in range(sensor_size[1] // K +1):\n",
    "                xs = events[:,x_index]\n",
    "                ys = events[:,y_index]\n",
    "                selection = events[(xs >= i*K) & (xs < i*K+K) & (ys >= j*K) & (ys < j*K+K)]\n",
    "                if len(selection) > 0:\n",
    "                    cells.extend([cell_index] * len(selection))\n",
    "                cell_index += 1\n",
    "        \n",
    "        hists = []\n",
    "        for i in np.arange(n_cells):\n",
    "            selection = (cells == i)\n",
    "            if len(selection) > 0:\n",
    "                hists.append(np.histogram(Y_pred[selection], bins=np.arange(0, n_nodes[inference_level-1]+1))[0])\n",
    "            else:\n",
    "                hists.append(np.zeros(n_nodes[inference_level-1]))\n",
    "        X.append(np.concatenate(hists))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fb03d",
   "metadata": {},
   "source": [
    "### pytorch classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logreg import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# creating histograms from hard clusters\n",
    "if spatial_histograms:\n",
    "    train_features, train_labels = create_spatial_histograms(trainloader, model, n_nodes, inference_level, train_set, K)\n",
    "    test_features, test_labels = create_spatial_histograms(testloader, model, n_nodes, inference_level, test_set, K)\n",
    "else:\n",
    "    train_features, train_labels = create_histograms(trainloader, model, n_nodes, inference_level, train_set)\n",
    "    test_features, test_labels = create_histograms(testloader, model, n_nodes, inference_level, test_set)\n",
    "\n",
    "# scale features to 0 mean and 1 variance\n",
    "scaler = preprocessing.StandardScaler().fit(train_features)\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "# creating dataloaders\n",
    "training_dataset = TensorDataset(torch.Tensor(train_features),torch.Tensor(train_labels))\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=128)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(test_features),torch.Tensor(test_labels))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# finding unique classes\n",
    "classes = np.unique(test_labels)\n",
    "\n",
    "# training pytorch logistic regression\n",
    "logreg = LogisticRegression(train_features.shape[1], len(classes), epochs=200, lr=0.01, step_size=30, gamma=1, momentum=0, weight_decay=0)\n",
    "logreg.fit(training_dataloader)\n",
    "score = logreg.score(test_dataloader)*100\n",
    "\n",
    "# print score\n",
    "print(score)\n",
    "\n",
    "# save results\n",
    "p = Path('benchmark_hgmm')\n",
    "p.mkdir(exist_ok=True)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
    "filename = dataset_name+'_'+str(score)+'_'+str(n_nodes[inference_level-1])+'_sp-hist_'+str(spatial_histograms)+'_coreset_'+str(coresets)+'_'+date\n",
    "np.save(p/filename, score)\n",
    "\n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a40f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
